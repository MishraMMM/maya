#ANN Assignment 4

import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, input_dim, lr=0.1):
        self.weights = np.random.randn(input_dim + 1)
        self.lr = lr

    def predict(self, inputs):
        inputs = np.append(inputs, 1.0)
        activation = np.dot(self.weights, inputs)
        return np.where(activation > 0, 1, -1)

    def train(self, inputs, targets, epochs):
        for epoch in range(epochs):
            for i, x in enumerate(inputs):
                target = targets[i]
                prediction = self.predict(x)
                if prediction != target:
                    error = target - prediction
                    inputs_with_bias = np.append(x, 1.0)
                    self.weights += self.lr * error * inputs_with_bias

    def plot_decision_regions(self, inputs, targets):
        x_min, x_max = inputs[:, 0].min() - 1, inputs[:, 0].max() + 1
        y_min, y_max = inputs[:, 1].min() - 1, inputs[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))
        Z = np.array([self.predict(np.array([x, y])) for x, y in np.c_[xx.ravel(), yy.ravel()]])
        Z = Z.reshape(xx.shape)
        plt.contourf(xx, yy, Z, alpha=0.4)
        plt.scatter(inputs[:, 0], inputs[:, 1], c=targets, edgecolors='black')
        plt.show()

# example usage
inputs = np.array([[1, 2], [2, 3],[3,5], [3, 1], [4, 3]])
targets = np.array([1, 1, 1, -1, -1])
perceptron = Perceptron(input_dim=2)
perceptron.train(inputs, targets, epochs=10)
perceptron.plot_decision_regions(inputs, targets)


import numpy as np
import matplotlib.pyplot as plt

input_dim=2
weights = np.random.randn(input_dim + 1)
lr=0.1

def predict(inputs):
        inputs = np.append(inputs, 1.0)
        activation = np.dot(weights, inputs)
        return np.where(activation > 0, 1, -1)

def train(inputs, targets, epochs):
        for epoch in range(epochs):
            for i, x in enumerate(inputs):
                target = targets[i]
                prediction = predict(x)
                if prediction != target:
                    error = target - prediction
                    inputs_with_bias = np.append(x, 1.0)
                    weights += lr * error * inputs_with_bias

def plot_decision_regions(inputs, targets):
      x_min, x_max = inputs[:, 0].min() - 1, inputs[:, 0].max() + 1
      y_min, y_max = inputs[:, 1].min() - 1, inputs[:, 1].max() + 1
      xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))
      Z = np.array([predict(np.array([x, y])) for x, y in np.c_[xx.ravel(), yy.ravel()]])
      Z = Z.reshape(xx.shape)
      plt.contourf(xx, yy, Z, alpha=0.4)
      plt.scatter(inputs[:, 0], inputs[:, 1], c=targets, edgecolors='black')
      plt.show()


# example usage
inputs = np.array([[1, 2], [2, 3], [3, 1], [4, 3]])
targets = np.array([1, 1, -1, -1])

# perceptron = Perceptron(input_dim=2)
train(inputs, targets, epochs=10)
plot_decision_regions(inputs, targets)

from sklearn import datasets
X, y = datasets.make_blobs(n_samples=150,n_features=2,
                           centers=2,cluster_std=1.05,
                           random_state=2)

def step_func(z):
        return 1.0 if (z > 0) else 0.0

def perceptron(X, y, lr, epochs): 
    m, n = X.shape
    
    theta = np.zeros((n+1,1))
    
    # Empty list to store how many examples were 
    # misclassified at every iteration.
    n_miss_list = []
    
    # Training.
    for epoch in range(epochs):
        
        # variable to store #misclassified.
        n_miss = 0
        
        # looping for every example.
        for idx, x_i in enumerate(X):
            
            # Insering 1 for bias, X0 = 1.
            x_i = np.insert(x_i, 0, 1).reshape(-1,1)
            
            # Calculating prediction/hypothesis.
            y_hat = step_func(np.dot(x_i.T, theta))
            
            # Updating if the example is misclassified.
            if (np.squeeze(y_hat) - y[idx]) != 0:
                theta += lr*((y[idx] - y_hat)*x_i)
                
                # Incrementing by 1.
                n_miss += 1
        
        # Appending number of misclassified examples
        # at every iteration.
        n_miss_list.append(n_miss)
        
    return theta, n_miss_list


def plot_decision_boundary(X, theta):
  
    x1 = [min(X[:,0]), max(X[:,0])]
    m = -theta[1]/theta[2]
    c = -theta[0]/theta[2]
    x2 = m*x1 + c
    
    # Plotting
    fig = plt.figure(figsize=(10,8))
    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "r^")
    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs")
    # plt.xlabel("feature 1")
    # plt.ylabel("feature 2")
    # plt.title('Perceptron Algorithm')
    plt.plot(x1, x2, 'y-')
    plt.show()

theta, miss_l = perceptron(X, y, 0.5, 100)
plot_decision_boundary(X, theta)

m, n = X.shape
n

np.zeros((n+1,1))

